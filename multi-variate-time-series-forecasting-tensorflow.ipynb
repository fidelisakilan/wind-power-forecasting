{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:47.041480Z",
     "iopub.status.busy": "2020-10-28T21:43:47.040602Z",
     "iopub.status.idle": "2020-10-28T21:43:52.776271Z",
     "shell.execute_reply": "2020-10-28T21:43:52.774654Z"
    },
    "papermill": {
     "duration": 5.766168,
     "end_time": "2020-10-28T21:43:52.776444",
     "exception": false,
     "start_time": "2020-10-28T21:43:47.010276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:52.860420Z",
     "iopub.status.busy": "2020-10-28T21:43:52.859428Z",
     "iopub.status.idle": "2020-10-28T21:43:53.205379Z",
     "shell.execute_reply": "2020-10-28T21:43:53.207050Z"
    },
    "papermill": {
     "duration": 0.393601,
     "end_time": "2020-10-28T21:43:53.207263",
     "exception": false,
     "start_time": "2020-10-28T21:43:52.813662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate Sample\n",
      "      PS  WS50M  WS10M    T2M\n",
      "0  98.49   6.54   3.38  18.79\n",
      "1  98.57   5.91   3.48  19.26\n",
      "2  98.66   5.74   4.70  21.07\n",
      "3  98.72   7.67   6.18  22.94\n",
      "4  98.72   8.40   6.94  24.27\n"
     ]
    }
   ],
   "source": [
    "def custom_date_parser(x): return datetime.strptime(x, \"%Y %m %d %H\")\n",
    "\n",
    "def load_data(col=None, path=\"MUP_dataset.csv\", verbose=False):\n",
    "    df = pd.read_csv(path, header=12)\n",
    "    dateCols=[\"YEAR\",\"MO\",\"DY\",'HR']\n",
    "    # df['time'] =df[dateCols].apply(lambda x:datetime.strptime(\"{0} {1} {2} {3}:00:00\".format(x['YEAR'],x['MO'], x['DY'],x['HR']), \"%Y %m %d %H:%M:%S\"),axis=1)\n",
    "    df['time'] =df[dateCols].apply(lambda x:\"{0}-{1}-{2} {3}:00:00\".format(x['YEAR'],x['MO'], x['DY'],x['HR']),axis=1)    \n",
    "    df.drop(dateCols,axis=1)\n",
    "    if col is not None:\n",
    "        df = df[col]\n",
    "    if verbose:\n",
    "        print(df.head())\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Multivariate Sample\")\n",
    "multivar_df = load_data(\n",
    "    ['PS', 'WS50M', 'WS10M', 'T2M'], verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03672,
     "end_time": "2020-10-28T21:43:53.287100",
     "exception": false,
     "start_time": "2020-10-28T21:43:53.250380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Baseline Error Performance\n",
    "\n",
    "In order to compare model perofrmance we need an estimate of bayes limit for the problem. In this case we do not have a human error reference. So we use the the lowest of the following:\n",
    "- ENTSOE recorded forecast. This is the collection of models used by the relevant energy authority.\n",
    "- Persistance 1 Day. Using the observed values from the previous days as the prediction of the next day.\n",
    "- Persistance 3 day mean. Using the observations from the previous 3 days as the prediction of the next day.\n",
    "\n",
    "By establishing a baseline error we have a refernce to compare our training and validation set performance. This guides us to understand where and how a model is performance. For example, if our bayes error is MAE 5% and our model training and validation perform at MAE 6% and 9% respectively then the relevant obserevation is that our model performs with high variance with respect to the baseline. The contrary is true if we consider baseline, train, and validation MAEs of 5%, 8%, and 8.5% respectively. In the latter case we should work on the bias of the training set before considering the validation performance (low variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:53.371178Z",
     "iopub.status.busy": "2020-10-28T21:43:53.370117Z",
     "iopub.status.idle": "2020-10-28T21:43:56.241457Z",
     "shell.execute_reply": "2020-10-28T21:43:56.240812Z"
    },
    "papermill": {
     "duration": 2.918028,
     "end_time": "2020-10-28T21:43:56.241591",
     "exception": false,
     "start_time": "2020-10-28T21:43:53.323563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = load_data(col=[\"WS50M\"])\n",
    "\n",
    "# # fill nans with linear interpolation because this is how we will fill when using the data in the models.\n",
    "# df_filled = df.interpolate(\"linear\")\n",
    "# mm = MinMaxScaler()\n",
    "# df_scaled = mm.fit_transform(df_filled)\n",
    "\n",
    "# df_prep = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "# y_true = df_prep[\"WS50M\"]\n",
    "# y_pred_forecast = df_prep[\"WS50M\"].shift(-1)\n",
    "# y_true = y_true[:-1]\n",
    "# y_pred_forecast = y_pred_forecast[:-1]\n",
    "# # persistence 1 day\n",
    "# # shift series by 24 hours\n",
    "# # realign y_true to have the same length and time samples\n",
    "# y_preds_persistance_1_day = y_true.shift(24).dropna()\n",
    "# persistence_1_day_mae = tf.keras.losses.MAE(\n",
    "#     y_true[y_preds_persistance_1_day.index], y_preds_persistance_1_day).numpy()\n",
    "# persistence_1_day_mape = tf.keras.losses.MAPE(np.maximum(\n",
    "#     y_true[y_preds_persistance_1_day.index], 1e-5), np.maximum(y_preds_persistance_1_day, 1e-5)).numpy()\n",
    "\n",
    "# # persistence 3 day average\n",
    "# # shift by 1, 2, 3 days. Realign to have same lengths. Average days and calcualte MAE.\n",
    "\n",
    "# shift_dfs = list()\n",
    "# for i in range(1, 4):\n",
    "#     shift_dfs.append(pd.Series(y_true.shift(24 * i), name=f\"d{i}\"))\n",
    "\n",
    "# y_persistance_3d = pd.concat(shift_dfs, axis=1).dropna()\n",
    "# y_persistance_3d[\"avg\"] = (y_persistance_3d[\"d1\"] +\n",
    "#                            y_persistance_3d[\"d2\"] + y_persistance_3d[\"d3\"])/3\n",
    "# d3_idx = y_persistance_3d.index\n",
    "# persistence_3day_avg_mae = tf.keras.losses.MAE(\n",
    "#     y_true[d3_idx], y_persistance_3d['avg']).numpy()\n",
    "# persistence_3day_avg_mape = tf.keras.losses.MAPE(np.maximum(\n",
    "#     y_true[d3_idx], 1e-5), np.maximum(y_persistance_3d['avg'], 1e-5)).numpy()\n",
    "\n",
    "# ref_error = pd.DataFrame({\n",
    "#     \"Method\": [\"TSO Forecast\", \"Persistence 1 Day\", \"Persitence 3 Day Avg\"],\n",
    "#     \"MAE\": [tf.keras.losses.MAE(y_true, y_pred_forecast).numpy(),\n",
    "#             persistence_1_day_mae,\n",
    "#             persistence_3day_avg_mae],\n",
    "#     \"MAPE\": [tf.keras.losses.MAPE(np.maximum(y_true, 1e-5), np.maximum(y_pred_forecast, 1e-5)).numpy(),\n",
    "#              persistence_1_day_mape,\n",
    "#              persistence_3day_avg_mape],\n",
    "#              },\n",
    "#     index=[i for i in range(3)])\n",
    "\n",
    "# print(\"\\nSummary of Baseline Errors\")\n",
    "# print(ref_error)\n",
    "# print(\n",
    "#     f\"\\nAverage error of Wind Speed 50M in m/s: {round(df['WS50M'].shift(-1).mean()*ref_error.iloc[0,1], 2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029444,
     "end_time": "2020-10-28T21:43:56.361013",
     "exception": false,
     "start_time": "2020-10-28T21:43:56.331569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparing the Data\n",
    "\n",
    "We will use tf.datasets to prepare the data. The general strategy is to clean, scale, and split the data before creating the tf.dataset object. These steps can alternatively be done within the tf.dataset itself.\n",
    "\n",
    "***Cleaning data:*** Fill any missing values with a linear interpolation of the value. Same as done in the persistence dataset.\n",
    "\n",
    "***Scaling data:*** In all cases the data is min max scaled.\n",
    "\n",
    "***Features:*** As part of this simple analysis of models two feature sets are prepared. The univariate that contains energy consumption data only. The multivariate that contains energy consumption, price, day of the week, and month of the year.\n",
    "\n",
    "***Splitting data:*** One year of test data (8769 hourly samples) is put aside to evaluate all the models. The train and validation sets are created with a 65/35 split, resulting in 9207 validation samples - just over one year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:56.430046Z",
     "iopub.status.busy": "2020-10-28T21:43:56.428949Z",
     "iopub.status.idle": "2020-10-28T21:43:56.433062Z",
     "shell.execute_reply": "2020-10-28T21:43:56.432356Z"
    },
    "papermill": {
     "duration": 0.041722,
     "end_time": "2020-10-28T21:43:56.433197",
     "exception": false,
     "start_time": "2020-10-28T21:43:56.391475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(series):\n",
    "    \"\"\"Fills missing values. \n",
    "    \n",
    "        Interpolate missing values with a linear approximation.\n",
    "    \"\"\"\n",
    "    series_filled = series.interpolate(method='linear')\n",
    "        \n",
    "    return series_filled\n",
    "        \n",
    "    \n",
    "def min_max_scale(dataframe):\n",
    "    \"\"\" Applies MinMax Scaling\n",
    "    \n",
    "        Wrapper for sklearn's MinMaxScaler class.\n",
    "    \"\"\"\n",
    "    mm = MinMaxScaler()\n",
    "    return mm.fit_transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:56.628190Z",
     "iopub.status.busy": "2020-10-28T21:43:56.627045Z",
     "iopub.status.idle": "2020-10-28T21:43:56.694748Z",
     "shell.execute_reply": "2020-10-28T21:43:56.695338Z"
    },
    "papermill": {
     "duration": 0.109048,
     "end_time": "2020-10-28T21:43:56.695501",
     "exception": false,
     "start_time": "2020-10-28T21:43:56.586453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(series, train_fraq, test_len=8760):\n",
    "    \"\"\"Splits input series into train, val and test.\n",
    "    \n",
    "        Default to 1 year of test data.\n",
    "    \"\"\"\n",
    "    #slice the last year of data for testing 1 year has 8760 hours\n",
    "    test_slice = len(series)-test_len\n",
    "\n",
    "    test_data = series[test_slice:]\n",
    "    train_val_data = series[:test_slice]\n",
    "\n",
    "    #make train and validation from the remaining\n",
    "    train_size = int(len(train_val_data) * train_fraq)\n",
    "    \n",
    "    train_data = train_val_data[:train_size]\n",
    "    val_data = train_val_data[train_size:]\n",
    "    \n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivarate Datasets\n",
      "Train Data Shape: (28501, 4)\n",
      "Val Data Shape: (15347, 4)\n",
      "Test Data Shape: (8760, 4)\n",
      "Nulls In Train False\n",
      "Nulls In Validation False\n",
      "Nulls In Test False\n"
     ]
    }
   ],
   "source": [
    "multivar_df = clean_data(multivar_df)\n",
    "\n",
    "# add hour and month features\n",
    "# hours, day, months = make_time_features(multivar_df.time)\n",
    "# multivar_df = pd.concat([multivar_df.drop(['time'], axis=1), hours, day, months], axis=1)\n",
    "\n",
    "#scale\n",
    "multivar_df = min_max_scale(multivar_df)\n",
    "train_multi, val_multi, test_multi = split_data(multivar_df, train_fraq=0.65, test_len=8760)\n",
    "print(\"Multivarate Datasets\")\n",
    "print(f\"Train Data Shape: {train_multi.shape}\")\n",
    "print(f\"Val Data Shape: {val_multi.shape}\")\n",
    "print(f\"Test Data Shape: {test_multi.shape}\")\n",
    "print(f\"Nulls In Train {np.any(np.isnan(train_multi))}\")\n",
    "print(f\"Nulls In Validation {np.any(np.isnan(val_multi))}\")\n",
    "print(f\"Nulls In Test {np.any(np.isnan(test_multi))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027967,
     "end_time": "2020-10-28T21:43:56.752086",
     "exception": false,
     "start_time": "2020-10-28T21:43:56.724119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Windowing the Dataset\n",
    "\n",
    "Use tf.dataset to create a window dataset. This is a vector of past timesteps (n_steps) that is used to predict on a target vector of future steps (n_horizon). The example below shows the output for n_steps = 72 and n_horizon = 24 and the 5 features. So we use the last 3 days (72 hours) to predict the next day (following 24 hours). \n",
    "\n",
    "The resulting shape for X will be (batch size, n_steps, features) and Y will be (batch size, n_horizon, features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:56.826032Z",
     "iopub.status.busy": "2020-10-28T21:43:56.825118Z",
     "iopub.status.idle": "2020-10-28T21:43:57.065802Z",
     "shell.execute_reply": "2020-10-28T21:43:57.066564Z"
    },
    "papermill": {
     "duration": 0.286687,
     "end_time": "2020-10-28T21:43:57.066778",
     "exception": false,
     "start_time": "2020-10-28T21:43:56.780091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.ShuffleDataset'>\n",
      "Example sample shapes\n",
      "x =  (1, 72, 4)\n",
      "y =  (1, 24, 1)\n"
     ]
    }
   ],
   "source": [
    "def window_dataset(data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=False, expand_dims=False):\n",
    "    \"\"\" Create a windowed tensorflow dataset\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #create a window with n steps back plus the size of the prediction length\n",
    "    window = n_steps + n_horizon\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    #create the window function shifting the data by the prediction length\n",
    "    ds = ds.window(window, shift=n_horizon, drop_remainder=True)\n",
    "    \n",
    "    #flatten the dataset and batch into the window size\n",
    "    ds = ds.flat_map(lambda x : x.batch(window))\n",
    "    ds = ds.shuffle(shuffle_buffer)    \n",
    "    print(type(ds))\n",
    "    #create the supervised learning problem x and y and batch\n",
    "    if multi_var:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:, :1]))\n",
    "    else:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:]))\n",
    "    \n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_steps = 72\n",
    "n_horizon = 24\n",
    "batch_size = 1\n",
    "shuffle_buffer = 100\n",
    "\n",
    "\n",
    "ds = window_dataset(train_multi, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=True)\n",
    "\n",
    "print('Example sample shapes')\n",
    "for idx,(x,y) in enumerate(ds):\n",
    "    print(\"x = \", x.numpy().shape)\n",
    "    print(\"y = \", y.numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029037,
     "end_time": "2020-10-28T21:43:57.126183",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.097146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Loading Function\n",
    "\n",
    "Wrap the above functions into a single function that allows us to build the dataset in the same way each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:57.204823Z",
     "iopub.status.busy": "2020-10-28T21:43:57.203757Z",
     "iopub.status.idle": "2020-10-28T21:43:57.477881Z",
     "shell.execute_reply": "2020-10-28T21:43:57.479119Z"
    },
    "papermill": {
     "duration": 0.323839,
     "end_time": "2020-10-28T21:43:57.479279",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.155440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dataset(train_fraq=0.65, \n",
    "                  n_steps=24*30, \n",
    "                  n_horizon=24, \n",
    "                  batch_size=256, \n",
    "                  shuffle_buffer=500, \n",
    "                  expand_dims=False, \n",
    "                  multi_var=False):\n",
    "    \"\"\"If multi variate then first column is always the column from which the target is contstructed.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.random.set_seed(23)\n",
    "    \n",
    "    if multi_var:\n",
    "        data = load_data(col=['PS', 'WS50M', 'WS10M', 'T2M'])\n",
    "        # hours, day, months = make_time_features(data.time)\n",
    "        # data = pd.concat([data.drop(['time'], axis=1), hours, day, months], axis=1)\n",
    "    else:\n",
    "        data = load_data(col=['WS50M'])\n",
    "        \n",
    "    data = clean_data(data)\n",
    "    \n",
    "    if multi_var:\n",
    "        mm = MinMaxScaler()\n",
    "        data = mm.fit_transform(data)\n",
    "    \n",
    "    train_data, val_data, test_data = split_data(data, train_fraq=train_fraq, test_len=8760)\n",
    "    \n",
    "    train_ds = window_dataset(train_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multi_var, expand_dims=expand_dims)\n",
    "    val_ds = window_dataset(val_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multi_var, expand_dims=expand_dims)\n",
    "    test_ds = window_dataset(test_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multi_var, expand_dims=expand_dims)\n",
    "    \n",
    "    \n",
    "    print(f\"Prediction lookback (n_steps): {n_steps}\")\n",
    "    print(f\"Prediction horizon (n_horizon): {n_horizon}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(\"Datasets:\")\n",
    "    print(train_ds.element_spec)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "train_ds, val_ds, test_ds = build_dataset(multi_var=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029697,
     "end_time": "2020-10-28T21:43:57.538707",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.509010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Configurations\n",
    "\n",
    "Define a set of model configurations so that we can call and run each model in the same way. The cgf_model_run dictionary will store the model, its history, and the test datasetset generated.\n",
    "\n",
    "The default model parameters are:\n",
    "- n_steps: last 30 days\n",
    "- n_horizon: next 24 hours\n",
    "- learning rate: 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:57.610369Z",
     "iopub.status.busy": "2020-10-28T21:43:57.609393Z",
     "iopub.status.idle": "2020-10-28T21:43:57.612573Z",
     "shell.execute_reply": "2020-10-28T21:43:57.613111Z"
    },
    "papermill": {
     "duration": 0.045045,
     "end_time": "2020-10-28T21:43:57.613253",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.568208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_params(multivar=False):\n",
    "    lr = 3e-4\n",
    "    n_steps=24*30\n",
    "    n_horizon=24\n",
    "    if multivar:\n",
    "        n_features=4\n",
    "    else:\n",
    "        n_features=1\n",
    "        \n",
    "    return n_steps, n_horizon, n_features, lr\n",
    "\n",
    "model_configs = dict()\n",
    "\n",
    "def cfg_model_run(model, history, test_ds):\n",
    "    return {\"model\": model, \"history\" : history, \"test_ds\": test_ds}\n",
    "\n",
    "\n",
    "def run_model(model_name, model_func, model_configs, epochs):\n",
    "    \n",
    "    n_steps, n_horizon, n_features, lr = get_params(multivar=True)\n",
    "    train_ds, val_ds, test_ds = build_dataset(n_steps=n_steps, n_horizon=n_horizon, multi_var=True)\n",
    "\n",
    "    model = model_func(n_steps, n_horizon, n_features, lr=lr)\n",
    "\n",
    "    model_hist = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "\n",
    "    model_configs[model_name] = cfg_model_run(model, model_hist, test_ds)\n",
    "    return test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030619,
     "end_time": "2020-10-28T21:43:57.673713",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.643094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Each Model\n",
    "\n",
    "## DNN\n",
    "A single 128 unit layer plus the common 128 and 24 unit layyers with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:57.746324Z",
     "iopub.status.busy": "2020-10-28T21:43:57.745392Z",
     "iopub.status.idle": "2020-10-28T21:43:57.842303Z",
     "shell.execute_reply": "2020-10-28T21:43:57.841437Z"
    },
    "papermill": {
     "duration": 0.138441,
     "end_time": "2020-10-28T21:43:57.842474",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.704033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(n_steps, n_features)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(n_horizon)\n",
    "    ], name='dnn')\n",
    "    \n",
    "    loss=tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "dnn = dnn_model(*get_params(multivar=True))\n",
    "dnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031041,
     "end_time": "2020-10-28T21:43:57.904342",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.873301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CNN\n",
    "Two Conv 1D layers with 64 filters each, and kernel sizes of 6 and 3 respectively. After each Conv1D layer a maxpooling1D layer with size of 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:57.978118Z",
     "iopub.status.busy": "2020-10-28T21:43:57.977126Z",
     "iopub.status.idle": "2020-10-28T21:43:58.076456Z",
     "shell.execute_reply": "2020-10-28T21:43:58.077463Z"
    },
    "papermill": {
     "duration": 0.143364,
     "end_time": "2020-10-28T21:43:58.077647",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.934283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cnn_model(n_steps, n_horizon, n_features, lr=3e-4):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features)),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(n_horizon)\n",
    "    ], name=\"CNN\")\n",
    "    \n",
    "    loss= tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "cnn = cnn_model(*get_params(multivar=True))\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03078,
     "end_time": "2020-10-28T21:43:58.140097",
     "exception": false,
     "start_time": "2020-10-28T21:43:58.109317",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LSTM\n",
    "Two LSTM layers with 72 and 48 units each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:58.215969Z",
     "iopub.status.busy": "2020-10-28T21:43:58.214120Z",
     "iopub.status.idle": "2020-10-28T21:43:58.459042Z",
     "shell.execute_reply": "2020-10-28T21:43:58.458323Z"
    },
    "papermill": {
     "duration": 0.28786,
     "end_time": "2020-10-28T21:43:58.459177",
     "exception": false,
     "start_time": "2020-10-28T21:43:58.171317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lstm_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(100, activation='relu', input_shape=(n_steps, n_features)),\n",
    "        # tf.keras.layers.LSTM(48, activation='relu', return_sequences=False),\n",
    "        # tf.keras.layers.Flatten(),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(50),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(n_horizon)\n",
    "    ], name='lstm')\n",
    "    \n",
    "    loss = tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "lstm = lstm_model(*get_params(multivar=True))\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031435,
     "end_time": "2020-10-28T21:43:58.523084",
     "exception": false,
     "start_time": "2020-10-28T21:43:58.491649",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CNN and LSTM Stacked\n",
    "Using the same layers from the CNN and LSTM model, stack the CNN as input to the pair of LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:58.622523Z",
     "iopub.status.busy": "2020-10-28T21:43:58.600345Z",
     "iopub.status.idle": "2020-10-28T21:43:58.832292Z",
     "shell.execute_reply": "2020-10-28T21:43:58.833511Z"
    },
    "papermill": {
     "duration": 0.279217,
     "end_time": "2020-10-28T21:43:58.833748",
     "exception": false,
     "start_time": "2020-10-28T21:43:58.554531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lstm_cnn_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=6, activation='relu', input_shape=(n_steps,n_features)),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        tf.keras.layers.LSTM(72, activation='relu', return_sequences=True),\n",
    "        tf.keras.layers.LSTM(48, activation='relu', return_sequences=False),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(n_horizon)\n",
    "    ], name=\"lstm_cnn\")\n",
    "    \n",
    "    loss = tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "lstm_cnn = lstm_cnn_model(*get_params(multivar=True))\n",
    "lstm_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031768,
     "end_time": "2020-10-28T21:43:58.898034",
     "exception": false,
     "start_time": "2020-10-28T21:43:58.866266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CNN and LSTM with a skip connection\n",
    "The same CNN and LSTM layers as the previous models this time with a skip connection direct to the common DNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:58.987682Z",
     "iopub.status.busy": "2020-10-28T21:43:58.980953Z",
     "iopub.status.idle": "2020-10-28T21:43:59.218962Z",
     "shell.execute_reply": "2020-10-28T21:43:59.220246Z"
    },
    "papermill": {
     "duration": 0.289637,
     "end_time": "2020-10-28T21:43:59.220454",
     "exception": false,
     "start_time": "2020-10-28T21:43:58.930817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lstm_cnn_skip_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "   \n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps,n_features), name='main')\n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(64, kernel_size=6, activation='relu')(inputs)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(2)(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu')(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(2)(conv2)\n",
    "    lstm_1 = tf.keras.layers.LSTM(72, activation='relu', return_sequences=True)(max_pool_2)\n",
    "    lstm_2 = tf.keras.layers.LSTM(48, activation='relu', return_sequences=False)(lstm_1)\n",
    "    flatten = tf.keras.layers.Flatten()(lstm_2)\n",
    "    \n",
    "    skip_flatten = tf.keras.layers.Flatten()(inputs)\n",
    "\n",
    "    concat = tf.keras.layers.Concatenate(axis=-1)([flatten, skip_flatten])\n",
    "    drop_1 = tf.keras.layers.Dropout(0.3)(concat)\n",
    "    dense_1 = tf.keras.layers.Dense(128, activation='relu')(drop_1)\n",
    "    drop_2 = tf.keras.layers.Dropout(0.3)(dense_1)\n",
    "    output = tf.keras.layers.Dense(n_horizon)(drop_2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='lstm_skip')\n",
    "    \n",
    "    loss = tf.keras.losses.Huber()\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "lstm_skip = lstm_cnn_skip_model(*get_params(multivar=True))\n",
    "lstm_skip.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:59.307335Z",
     "iopub.status.busy": "2020-10-28T21:43:59.293878Z",
     "iopub.status.idle": "2020-10-28T21:43:59.859109Z",
     "shell.execute_reply": "2020-10-28T21:43:59.859824Z"
    },
    "papermill": {
     "duration": 0.604236,
     "end_time": "2020-10-28T21:43:59.859974",
     "exception": false,
     "start_time": "2020-10-28T21:43:59.255738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(lstm_skip, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034985,
     "end_time": "2020-10-28T21:43:59.930877",
     "exception": false,
     "start_time": "2020-10-28T21:43:59.895892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Models\n",
    "Run each model for 150 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:44:00.012574Z",
     "iopub.status.busy": "2020-10-28T21:44:00.011564Z",
     "iopub.status.idle": "2020-10-28T22:19:33.964140Z",
     "shell.execute_reply": "2020-10-28T22:19:33.963601Z"
    },
    "papermill": {
     "duration": 2133.999074,
     "end_time": "2020-10-28T22:19:33.964255",
     "exception": false,
     "start_time": "2020-10-28T21:43:59.965181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_model(\"dnn\", dnn_model, model_configs, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(\"cnn\", cnn_model, model_configs, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(\"lstm\", lstm_model, model_configs, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(\"lstm_cnn\", lstm_cnn_model, model_configs, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(\"lstm_skip\", lstm_cnn_skip_model, model_configs, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.08048,
     "end_time": "2020-10-28T22:19:36.122646",
     "exception": false,
     "start_time": "2020-10-28T22:19:35.042166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation of Training/Validation Results\n",
    "\n",
    "Loss curves across the models are fairly stable. All models show a flat validation curve while training continues to decline. The LSTM appears to begin to become very overfit from about epoch 100 where the validation loss begins to rise. The lstm_skip also has a point around epoch 50 where the val loss stops decreasing. In all cases this is a sign the models are no longer learning against the validation set. Some options to help improve this are to introduce learning rate decline, or train on longer input sequences.\n",
    "\n",
    "Plots of the MAE show a similar pattern to the loss plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T22:19:38.504323Z",
     "iopub.status.busy": "2020-10-28T22:19:38.503423Z",
     "iopub.status.idle": "2020-10-28T22:19:39.313307Z",
     "shell.execute_reply": "2020-10-28T22:19:39.312557Z"
    },
    "papermill": {
     "duration": 2.101218,
     "end_time": "2020-10-28T22:19:39.313428",
     "exception": false,
     "start_time": "2020-10-28T22:19:37.212210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "legend = list()\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(25,5))\n",
    "\n",
    "def plot_graphs(metric, val, ax, upper):\n",
    "    ax.plot(val['history'].history[metric])\n",
    "    ax.plot(val['history'].history[f'val_{metric}'])\n",
    "    ax.set_title(key)\n",
    "    ax.legend([metric, f\"val_{metric}\"])\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim([0, upper])\n",
    "    \n",
    "for (key, val), ax in zip(model_configs.items(), axs.flatten()):\n",
    "    plot_graphs('loss', val, ax, 0.2)\n",
    "print(\"Loss Curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T22:19:41.598979Z",
     "iopub.status.busy": "2020-10-28T22:19:41.597929Z",
     "iopub.status.idle": "2020-10-28T22:19:42.325267Z",
     "shell.execute_reply": "2020-10-28T22:19:42.325838Z"
    },
    "papermill": {
     "duration": 1.858989,
     "end_time": "2020-10-28T22:19:42.325984",
     "exception": false,
     "start_time": "2020-10-28T22:19:40.466995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"MAE Curves\")\n",
    "fig, axs = plt.subplots(1, 5, figsize=(25,5))\n",
    "for (key, val), ax in zip(model_configs.items(), axs.flatten()):\n",
    "    plot_graphs('mae', val, ax, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.089627,
     "end_time": "2020-10-28T22:19:44.539053",
     "exception": false,
     "start_time": "2020-10-28T22:19:43.449426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation of Test Results\n",
    "The LSTM and the CNN stacked LSTM models clearly outperformed the other four models. Whats surprising is to see how well both a CNN and DNN did on their. LSTM would be expected to perform well because of its ability to learn and remember longer trends in the data.\n",
    "\n",
    "Comparing to the baseline results the models' performance was poor. The dnn, cnn, lstm, and lstm_cnn models improved against the persistence error (MAE ~ 0.106) but did not improve against the TSO's prediction error (MAE ~0.015, MW error ~443). \n",
    "\n",
    "Putting the models' performance in perspective however the results show how with a limited lookback window, and simple features a lstm, and a cnn stacked with an lstm are a good starting choice for architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T22:19:46.776532Z",
     "iopub.status.busy": "2020-10-28T22:19:46.775688Z",
     "iopub.status.idle": "2020-10-28T22:19:48.679343Z",
     "shell.execute_reply": "2020-10-28T22:19:48.678846Z"
    },
    "papermill": {
     "duration": 3.050471,
     "end_time": "2020-10-28T22:19:48.679451",
     "exception": false,
     "start_time": "2020-10-28T22:19:45.628980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "names = list()\n",
    "performance = list()\n",
    "\n",
    "for key, value in model_configs.items():\n",
    "    names.append(key)\n",
    "    mae = value['model'].evaluate(value['test_ds'])\n",
    "    performance.append(mae[1])\n",
    "    \n",
    "performance_df = pd.DataFrame(performance, index=names, columns=['mae'])\n",
    "performance_df['error_mw'] = performance_df['mae'] * multivar_df['WS50M'].mean()\n",
    "print(performance_df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.110259,
     "end_time": "2020-10-28T22:19:51.131105",
     "exception": false,
     "start_time": "2020-10-28T22:19:50.020846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualizing Predictions\n",
    "\n",
    "Plot the actual and predicted 24 hour intervals. Below is the first 14 days of predictions. Interesting to note how the LSTM appears to oscilate over a longer frequency compared with the other models. The CNN also seems to capture the intra day oscillations (within the 24 hour period). Looking at the CNN stacked LSTM we can see how these two characteristics of the model's learning combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T22:19:53.359148Z",
     "iopub.status.busy": "2020-10-28T22:19:53.357840Z",
     "iopub.status.idle": "2020-10-28T22:19:57.995535Z",
     "shell.execute_reply": "2020-10-28T22:19:57.996048Z"
    },
    "papermill": {
     "duration": 5.757425,
     "end_time": "2020-10-28T22:19:57.996184",
     "exception": false,
     "start_time": "2020-10-28T22:19:52.238759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 1, figsize=(18, 10))\n",
    "days = 14\n",
    "\n",
    "vline = np.linspace(0, days*24, days+1)\n",
    "\n",
    "for (key, val), ax in zip(model_configs.items(), axs):\n",
    "\n",
    "    test = val['test_ds']\n",
    "    preds = val['model'].predict(test)\n",
    "\n",
    "    xbatch, ybatch = iter(test).get_next()\n",
    "\n",
    "    ax.plot(ybatch.numpy()[:days].reshape(-1))\n",
    "    ax.plot(preds[:days].reshape(-1))\n",
    "    ax.set_title(key)\n",
    "    ax.vlines(vline, ymin=0, ymax=1, linestyle='dotted', transform = ax.get_xaxis_transform())\n",
    "    ax.legend([\"Actual\", \"Predicted\"])\n",
    "\n",
    "plt.xlabel(\"Hours Cumulative\")\n",
    "print('First Two Weeks of Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.662759,
     "end_time": "2020-10-28T22:20:00.757355",
     "exception": false,
     "start_time": "2020-10-28T22:19:59.094596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "1. [Sequences, Time Series and Prediction, Coursera by Laurence Moroney](https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "duration": 2181.254074,
   "end_time": "2020-10-28T22:20:03.921952",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-28T21:43:42.667878",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
